# ollama/ollama - Repository Information

**Generated:** 2025-11-02 23:17:23

---

## Repository Details

- **Name:** ollama
- **Full Name:** ollama/ollama
- **Description:** Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.
- **Stars:** 155232
- **Forks:** 13526
- **Language:** Go
- **Open Issues:** 2232
- **License:** MIT License
- **Created:** 2023-06-26T19:39:32Z
- **Updated:** 2025-11-02T22:03:59Z
- **Clone URL:** https://github.com/ollama/ollama.git
- **Homepage:** https://ollama.com
- **Default Branch:** main
- **Repository URL:** https://github.com/ollama/ollama

## Recent Commits

- **ggml: Avoid cudaMemsetAsync during memory fitting**
  - Author: Jesse Gross
  - Date: 2025-10-31T21:16:20Z
  - SHA: 392a270

- **cpu: always ensure LibOllamaPath included (#12890)**
  - Author: Daniel Hiltgen
  - Date: 2025-10-31T21:37:29Z
  - SHA: 3bee3af

- **logs: catch rocm errors (#12888)**
  - Author: Daniel Hiltgen
  - Date: 2025-10-31T16:54:25Z
  - SHA: 8353799

- **embeddings: removed redundant TestAPIEmbeddings test (#12863)**
  - Author: nicole pardal
  - Date: 2025-10-31T00:12:33Z
  - SHA: 7dd4862

- **win: avoid ID mixups on refresh (#12869)**
  - Author: Daniel Hiltgen
  - Date: 2025-10-30T22:12:14Z
  - SHA: db973c8

- **ggml: Enable op_offload to improve partial offload performance**
  - Author: Jesse Gross
  - Date: 2025-10-27T23:32:05Z
  - SHA: afaf7ce

- **ollamarunner: Worst case batch for token generation**
  - Author: Jesse Gross
  - Date: 2025-10-27T23:31:58Z
  - SHA: 26465fb

- **win: use copy for subprocess logs (#12864)**
  - Author: Daniel Hiltgen
  - Date: 2025-10-30T20:22:00Z
  - SHA: 88236bc

- **testing: test more models with tool calling (#12867)**
  - Author: Patrick Devine
  - Date: 2025-10-30T20:19:21Z
  - SHA: 76eb7d0

- **interleaved mrope (#12807)**
  - Author: Michael Yang
  - Date: 2025-10-30T18:29:00Z
  - SHA: f67a6df

## Open Issues

- **#12914:** server: add ThinkLevel capability
  - State: open

- **#12913:** Vulkan backend performance is slow (7.73 tokens/s) [bug]
  - State: open

- **#12912:** Something changed after 0.12.6 in memory management in a not good way [bug, memory]
  - State: open

- **#12911:** Problem witch loading models in vram
  - State: open

- **#12909:** Add bash-completions support [feature request]
  - State: open

- **#12908:** ROCM Library not found, wrong location? [bug]
  - State: open

- **#12907:** "/set nothink" not disabling reasoning output in Qwen3:4B [bug]
  - State: open

- **#12905:** Windows - Connection attemt failed [bug]
  - State: open

- **#12904:** docs: fix some of the openapi.yaml warnings
  - State: open

- **#12903:** Unable to use my AMD GPU with ollama on Ubuntu [bug, amd, needs more info, gpu]
  - State: open

## Recent Closed Issues

- **#12910:** [BUG] The qwen3-vl:30b-a3b-instruct model cannot output in parallel [bug]
  - State: closed

- **#12906:** qwen3-vl:30b think: false not working [bug]
  - State: closed

- **#12902:** macOS - Cannot use models outside of default directory in 0.12.9 release [bug, app]
  - State: closed

- **#12901:** Add gpu-2404 interface for ollama snap [feature request]
  - State: closed

- **#12892:** ggml: Avoid cudaMemsetAsync during memory fitting
  - State: closed

- **#12890:** cpu: always ensure LibOllamaPath included
  - State: closed

- **#12888:** logs: catch rocm errors
  - State: closed

- **#12886:** ollama 0.12.[78] not using AVX in non-GPU situations [bug]
  - State: closed

- **#12882:** 10x slower Qwen3 and 2.5 VL [bug]
  - State: closed

- **#12880:** [Nitpick] The ollama installer on windows ignors scalling settings. [bug]
  - State: closed
